{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM2Lg31knBiNPYPo8K+LjcA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"kMtKzosDxIf9"},"outputs":[],"source":["KNeighborsClassfier\n","  주요 파라미터\n","    n_neighbors = 정수값(default = 5)\n","    weights\n","      uniform > default+모든 이웃 동일 취급\n","      distance > 가까운 이웃 큰 영향\n","    p\n","      2 > 유클리드(default)\n","      1 > 맨해튼\n","from sklearn.neightbors import KNeighborsClassifier\n","model = KNeighborsClassifier(random_state = , p = , weights = ,)\n","\n","DecisionTree\n","  분류+회귀\n","  순수도가 높은 방향으로 이동\n","  지니 계수 > 낮은 값\n","  엔트로피 > 낮은 값\n","  주요 파라미터\n","    random_state\n","    criterion > givi or entropy\n","    min_samples_split > 최소 샘플수\n","    max_depth > 트리 깊이\n","\n","from sklearn.tree import DecisionTreeClassifer\n","model = DecisionTreeClassifer(random_state = 42, criterion = '', min_samples_split = , max_depth = ,)\n","\n","Ensemble\n","  개별 모델 > 결합해 예측\n","  오버피팅 감소\n","Voting\n","  분류, 회귀 가능\n","  하드 보팅 > 분류 > 최빈값\n","  소프트 보팅 > 회귀 > 예측 확률\n","  모델 객체 튜플 > 리스트로 담기\n","  주요파라미터\n","    estimator > 모델 객체 리스트 담기\n","    voting > hard or soft\n","from sklearn.ensemble import VotingClassifier\n","estimators = [\n","    ('별칭', 모델객체),\n","    ('별칭', 모델객체),\n","    ('별칭', 모델객체),\n","    etc\n","]\n","model = VotingClassifier(estimator = estimators, voting = '', n_jobs = -1)\n","\n","Stacking\n","  voting과 유사\n","  주요 파라미터\n","    estimators > 모델 객체 리스트 담기\n","    final_estimator > 메타 모델객체 전달(최종모델 선택)\n","from sklearn.ensemble import StackingClassifier\n","model = StackingClassifier(estimator = estimators, final_estimator = '', n_jobs = -1)\n","\n","Bagging\n","  학습데이터 n개 뽑아 학습 후 예측 결과 > 최빈값 or 평균 예측\n","  bootstrap sampling\n","  1개의 샘플 나올 확률 1/n > 새로운 학습데이터 > 복원 추출\n","  회귀, 분류 가능\n","  주요 파라미터\n","  random_state = 42\n","  n_estimators = 정수값 > 복원추출한 개수\n","from sklearn.ensemble import BaggingClassifier\n","BaggingClassifier(random_state = 42, n_estimator = 정수값, n_jobs = -1)\n","\n","RandomForest\n","  분류,회귀 가능\n","  배깅 방식 > 각 트리별 특성 선택\n","  주요 파라미터\n","    random_state = 42,\n","    n_estimators = 100(default)\n","    max_samples = None(default)\n","from sklearn.ensemble import RandomForestClassifier\n","model = RandomForestClassifier(random_state = 42, n_estimator = , max_samples = )\n","\n","GradientBoosting\n","  잔여 오차 > 새로운 학습 > 반복시 '0'에 가까움\n","  주요 파라미터\n","    random_state = 42\n","    n_estimators = 100(default)\n","    learning_rate = 0.1(default) >0~1값 > '1'에 가까울수록 오버피팅\n","from sklearn.ensemble import GradientBoostingClassfier\n","model = GradientBoostingClassfier(random_state = 42, n_estimators = , learning_rate = )\n","\n","XGBoost\n","  분류 회귀 가능\n","  주요 파라미터\n","    random_state = 42\n","    n_estimators = 100(default)\n","    learing_rate = 0.1(default)\n","    max_depth = 6(default)\n","from xgboost import XGBClassifier\n","model = XGBClassifier(random_state = 42, n_estimators = ,learning_rate = , max_depth)\n","\n","※학습 제어 관련 파라미터\n","  early_stopping_rounds : 정수값 > 정수값만큼 반복 시 개선 안되면 종료\n","  eval_metric > 검증에 대한 평가 지표\n","fit메소드의 파라미터\n","  eval_set = [(x_valid, y_valid)] > 검증셋 튜플에서 리스트로 담아 전달\n","\n","LightGBM\n","  데이터 많을 경우\n","  leaf 중심 트리분할 > 순수도가 높은 쪽으로만\n","  회귀, 분류 가능\n","  주요 파라미터\n","    random_state = 42\n","    n_estimators = 100(default)\n","    learning_rate = 0.1(default)\n","    max_depth = -1(default)\n","from lightgbm import LGBMClassifier\n","model = LGBMClassifier(random_state = 42, n_estimators = , max_depth = ,n_jobs = -1)\n","\n","※학습 제어 관련 파라미터\n","  early_stopping_round\n","  metric or\n","  fit 메소드 > eval_metric = 평가지표\n","\n","Catboost\n","  범주형 변수에 유용, 인코딩 x\n","  수치형 변수 > 범주형 변수 변환 후 학습\n","  주요 파라미터\n","    random_state = 42\n","    iterations = 100(default)\n","from catboost import CatBoostClassifier\n","model = CatBoostClassifier(ranom_state = 42, iterrations = , n_jobs = -1)\n","===========================================================================================\n","Featrue Engineering\n","  특성 변수 추출\n","  원본특성 조합+집계 > 새로운 특성 생성\n","  중요 feature끼리 사칙연산\n","  1개의 샘플+여러 행 구성 > 집계\n","\n","\n","agg_dict = {\n","    #key: 집계대상컬럼\n","    ('', '집계함수')\n","}\n","tmp = dataset.groupby('집계기준컬럼명').agg(agg_dict)\n","tmp.columns = tmp.columns.droplevel()\n","dataset = dataset.merge(tmp, on ='', how='left')\n","\n","Impute\n","  주요 파라미터\n","    strategy\n","      mean, median, most_frequent\n","      constant = fill_value\n","\n","    fill_value > 채우고 싶은 지정값\n","from sklearn.impute import SimpleImpute\n","model = SimpleImpute(strategy = '집계함수')\n","\n","Category_encoders\n","  다양한 방식 범주형\n","%pip install category_encoders\n","import category_encoders as ce\n","cols = 범주형 컬럼명 담기\n","\n","enc = ce.one_hot_OneHotEncoder()\n","enc = ce.count_countEncoder()\n","enc = ce.binary_BinaryEncoder()\n","\n","Combinations\n","  첫 번째 인수 : 반복가능한 객체 전달 > 해당 객체 내 n개의 가능한 모든 조합 튜플 조합\n","  두 번째 인수 : 인수로 정수n 전달 > n개씩 쌍을 이룸\n","from itertools import combinations\n","\n","for col in combinations(cols, 2): # 2개씩 쌍\n","  train_ft = train_ft[col[0]]+train_ft[col[1]]\n","\n","특성 인코딩\n","  수치형 데이터 입력\n","  범주형 > 수치형 변환\n","\n","Ordinal encoding\n","  범주형 변수 = 순서형 > 순서에 맞게 정수 부여\n","  dict로 처리\n","\n","Count encoding\n","  범주형 변수 > 각 고유값 빈도수 부여\n","  범주형 > 명목형 > 고유값 50 초과 시\n","\n","OneHot encdoing\n","  범주형 변수 > 고유값 개수만큼 열 데이터 생성 > 해당 범주 인덱스 '1', 아닌 범주 '0'\n","  고유값 개수 비례 feature차원 증가\n","\n","특성 스케일\n","MinMaxScaler\n","  feature별 최소값 '0', 최대값 '1'\n","  deep learning에서 유용\n","from sklearn.preprocessing import MinMaxScaler\n","\n","RobustScaler\n","  IQR Scaling\n","from sklearn.preproceesing import RobustScaler\n","\n","특성 선택\n","  중요한 feature 선택 후 학습\n","  feature 선별과정 필요\n","\n","SelectFromModel\n","  feature 중요도 or 가중치 기반 feature 선택\n","  특성선택할 모델 선정 > 모델학습 별도\n","from sklearn.feature_selection import SelectFromModel\n","fs = SelectFromModel(특성선택모델 객체)\n","\n","SelectKBest, SelectPercentile\n","  feature & target간 유의한 통계적 관계 계산 > feature 선택\n","  SelectKBest\n","    상위 n개 유의한 통계적 관계 feature선택\n","    SelectKBest(k = n) > 상위 n개 선택\n","\n","  SelectPercentile\n","    백분위수 ~ 유의한 통계적 관계 feature선택\n","    SelectPercentile(percentile = n) > 상위 n% 선택\n","from sklearn.feature_selection import SelectKBest, SelectPercentile\n"]}]}